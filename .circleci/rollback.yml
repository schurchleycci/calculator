# This is a template CircleCI rollback config. Uncomment and configure the section for your platform.
# NOTE: this template assumes the following:
# - your deployment is annotated with a "version" label
# - the name of your deployment or helm chart is the same as the name of the component. 
#   If this is not the case, you can instead add a label to the deployment with the component name and retrieve it that way

version: 2.1

orbs:
  aws-cli: circleci/aws-cli@5.4.0
  helm: circleci/helm@3.2.0

commands:
  # The following command is needed only for the specific logic in this example. Feel free to remove it if you don't need it.
  install_yq:
    steps:
      - run:
          name: install yq
          command: |
            wget https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64 -O /tmp/yq
            wget https://github.com/mikefarah/yq/releases/latest/download/checksums
            sha_file=$(sha256sum /tmp/yq | awk '{ print $1 }')
            sha=$(awk '$1=="yq_linux_amd64"{print $19}' checksums)

            if [ "$sha_file" != "$sha" ]; then
                    echo "Checksum failed" >&2
                    exit 1
            fi
            echo "The checksums match."
            chmod +x /tmp/yq

  verify_current_version:
    description: "Verifies that the currently deployed version matches the expected value"
    parameters:
      resource_name:
        type: string
        description: "Name of the resource to roll back"        
      namespace:
        type: string
        default: "default"
        description: "Kubernetes namespace (optional)"
      current_version:
        type: string
        description: "Current version"
    steps:
      - run:
          name: Verify current version
          command: |

            # --- Helm Example ------------------------------------------------------

            # RELEASE_NAME="<< parameters.resource_name >>"

            # if [ "<< parameters.current_version >>" == "" ]; then
            #   echo "Current version not specified."
            #   exit 0
            # fi

            # if [ -z "$RELEASE_NAME" ]; then
            #   echo "Missing release name"
            #   exit 1
            # fi

            # helm get manifest "<< parameters.resource_name >>" --namespace "<< parameters.namespace >>" > manifest.yaml

            # VERSION_LABEL=$(yq e '
            #   select(.kind == "Deployment") |
            #   .spec.template.metadata.labels.version
            # ' manifest.yaml)

            # if [ -z "$VERSION_LABEL" ] || [ "$VERSION_LABEL" == "null" ]; then
            #   echo "Could not extract version label from manifest"
            #   exit 1
            # fi

            # if [ "$VERSION_LABEL" == "<< parameters.current_version >>" ]; then
            #   echo "Version matches input version << parameters.current_version >>"
            # else
            #   echo "Version mismatch: expected << parameters.current_version >> but found $VERSION_LABEL"
            #   exit 1
            # fi

            # -----------------------------------------------------------------------





            # --- Kubectl Example ---------------------------------------------------

            # HISTORY_OUTPUT=$(kubectl rollout history deployment/<< parameters.resource_name >> -n << parameters.namespace >> --output=json)
            # if [[ -z $HISTORY_OUTPUT ]]; then
            #   echo "No rollout history found."
            #   exit 1
            # fi

            # # Extract revision numbers
            # REVISIONS=$(echo "$HISTORY_OUTPUT" | \
            #   jq -r '.metadata.annotations["deployment.kubernetes.io/revision"]')
      
            # if [[ -z "$REVISIONS" ]]; then
            #   echo "No revision annotations found in deployment metadata."
            #   exit 1
            # fi

            # MATCHING_REVISION=""
            # for REV in $REVISIONS; do
            #   TEMPLATE=$(kubectl rollout history deployment/<< parameters.resource_name >> -n << parameters.namespace >> --revision=$REV -o yaml 2>/dev/null)
  
            #   TEMPLATE_VERSION=$(echo "$TEMPLATE" | yq -r '.spec.template.metadata.labels.version' 2>/dev/null)

            #   if [[ $TEMPLATE_VERSION == << parameters.current_version >> ]]; then
            #     MATCHING_REVISION=$REV
            #     break
            #   fi
            # done

            # echo "export TARGET_REVISION=${MATCHING_REVISION}" >> $BASH_ENV
            # source $BASH_ENV    

            # -----------------------------------------------------------------------              


  retrieve_target_revision:
    description: "Retrieve previous version"
    parameters:
      resource_name:
        type: string
        description: "Name of the resource to roll back"       
      namespace:
        type: string
        default: "default"
        description: "Kubernetes namespace (optional)"
      target_version:
        type: string
        description: "Target version"        
    steps:    
      - run: 
          name: Identify previous revision
          command: |

            # --- Helm Example ------------------------------------------------------

            # TARGET_VERSION="<< parameters.target_version >>"
            # RELEASE_NAME="<< parameters.resource_name >>"
            # NAMESPACE="<< parameters.namespace >>"

            # if [ -z "$TARGET_VERSION" ]; then
            #   echo "TARGET_VERSION is required"
            #   exit 1
            # fi

            # # Get full release history
            # REVISIONS=$(helm history "$RELEASE_NAME" --namespace "$NAMESPACE" --output json | jq '.[].revision')

            # if [ -z "$REVISIONS" ]; then
            #   echo "Could not fetch Helm history for release '$RELEASE_NAME'"
            #   exit 1
            # fi

            # # Search each revision for a Deployment with the matching version label
            # TARGET_REVISION=""
            # for REV in $REVISIONS; do
            #   helm get manifest "$RELEASE_NAME" --namespace "$NAMESPACE" --revision "$REV" > manifest.yaml || continue

            #   VERSION_LABEL=$(yq e '
            #     select(.kind == "Deployment") |
            #     .spec.template.metadata.labels.version
            #   ' manifest.yaml)

            #   if [ "$VERSION_LABEL" == "$TARGET_VERSION" ]; then
            #     TARGET_REVISION=$REV
            #     break
            #   fi
            # done

            # if [ -n "$TARGET_REVISION" ]; then
            #   echo "export CONTAINER_VERSION=${TARGET_VERSION}" >> $BASH_ENV
            #   echo "export TARGET_REVISION=${TARGET_REVISION}" >> $BASH_ENV
            #   source $BASH_ENV
            # else
            #   echo "No revision found with version label: $TARGET_VERSION"
            #   exit 1
            # fi

            # -----------------------------------------------------------------------           





            # --- Kubectl Example ---------------------------------------------------

            # HISTORY_OUTPUT=$(kubectl rollout history deployment/<< parameters.resource_name >> -n << parameters.namespace >> --output=json)
            # if [[ -z $HISTORY_OUTPUT ]]; then
            #   echo "No rollout history found."
            #   exit 1
            # fi

            # # Extract revision numbers
            # REVISIONS=$(echo "$HISTORY_OUTPUT" | \
            #   jq -r '.metadata.annotations["deployment.kubernetes.io/revision"]')
      
            # if [[ -z "$REVISIONS" ]]; then
            #   echo "No revision annotations found in deployment metadata."
            #   exit 1
            # fi

            # echo "Searching for revision with version=<< parameters.target_version >>..."

            # MATCHING_REVISION=""
            # for REV in $REVISIONS; do
            #   TEMPLATE=$(kubectl rollout history deployment/<< parameters.resource_name >> -n << parameters.namespace >> --revision=$REV -o yaml 2>/dev/null)
  
            #   TEMPLATE_VERSION=$(echo "$TEMPLATE" | yq -r '.spec.template.metadata.labels.version' 2>/dev/null)

            #   if [[ $TEMPLATE_VERSION == << parameters.target_version >> ]]; then
            #     MATCHING_REVISION=$REV
            #     break
            #   fi
            # done

            # echo "Found matching revision: $MATCHING_REVISION for version: << parameters.target_version >>"
            # echo "export TARGET_REVISION=${MATCHING_REVISION}" >> $BASH_ENV
            # source $BASH_ENV

            # ----------------------------------------------------------------------- 


  perform_rollback:
    description: "perform rollback"
    parameters:
      resource_name:
        type: string
        description: "Name of the resource to roll back"        
      namespace:
        type: string
        default: "default"
        description: "Kubernetes namespace (optional)"
    steps:    
      - run: 
          name: Perform rollback
          command: |

            # --- Helm Example ------------------------------------------------------

            # helm rollback << parameters.resource_name >> ${TARGET_REVISION}

            # -----------------------------------------------------------------------





            # --- Kubectl Example ---------------------------------------------------

            # kubectl rollout undo deployment/<< parameters.resource_name >> -n << parameters.namespace >> --to-revision="$TARGET_REVISION"

            # -----------------------------------------------------------------------





            # --- Custom Script -----------------------------------------------------

            # Edit the rollback script path or add your custom rollback commands directly here
            # You can use pipeline values in your script like << pipeline.deploy.component_name >>
            
            # Example: Run a custom script
            # ./scripts/rollback-to-previous.sh

            # -----------------------------------------------------------------------


  # This command validates the deployment after rolling back. The provided example uses kubectl to check the ready replicas and number of restarts
  # of pods associated with the deployment and causes the job to fail if the deployment is not ready or has too many restarts by 
  # the end of the validation duration.
  # Mind the fact that the example assumes you have an app label with value equal to the component name, in order to retrieve the pods.
  # If that is not the case you will have to adapt the logic in the script.
  validate_deployment:
    description: "Validates the deployment after rolling back"
    parameters:
      resource_name:
        type: string
        description: "Name of the resource that has been rolled back"       
      namespace:
        type: string
        default: "default"
        description: "Kubernetes namespace (optional)"
      target_version:
        type: string
        description: "Target version"
      max_restarts:
        type: integer
        default: 5
        description: "Maximum number of allowed restarts"  
      duration:
        type: integer
        default: 600
        description: "Duration of the validation in seconds"                
    steps:    
      - run:
          name: Validate deployment
          command: |

            # --- Kubectl Example ---------------------------------------------------

            # CHECK_DURATION=$((SECONDS+<< parameters.duration >>))  # 10 minutes duration
            # REPLICAS_OK=false
            
            # LABEL_SELECTOR="app=<< parameters.resource_name >>,version=<< parameters.target_version >>"

            # DEPLOYMENT_FOUND=false

            # echo "Starting validation of version: << parameters.target_version >>"

            # while [ $SECONDS -lt $CHECK_DURATION ]; do

            #   DEPLOYMENT=$(kubectl get deployment << parameters.resource_name >>  -n << parameters.namespace >> --ignore-not-found -o json)
              
            #   if [ -n "$DEPLOYMENT" ]; then
            #       DEPLOYMENT_FOUND=true

            #       DESIRED=$(echo "$DEPLOYMENT" | jq -r '.spec.replicas // 0')
            #       READY=$(echo "$DEPLOYMENT" | jq -r '.status.readyReplicas // 0')
                  
            #       # Handle empty values
            #       DESIRED=${DESIRED:-0}
            #       READY=${READY:-0}
                  
            #       echo "Current replicas $READY/$DESIRED"

            #       if [ "$DESIRED" -eq "$READY" ]; then
            #         REPLICAS_OK=true
            #       else
            #         REPLICAS_OK=false
            #       fi
            #   else
            #       DEPLOYMENT_FOUND=false
            #       echo "Deployment not found"
            #       continue
            #   fi   

            #   RESTARTS=$(kubectl get pods -l $LABEL_SELECTOR -n << parameters.namespace >> \
            #     -o jsonpath='{.items[*].status.containerStatuses[*].restartCount}' 2>/dev/null | awk '{sum=0; for(i=1; i<=NF; i++) sum+=$i; print sum+0}')

            #   # Handle potential errors
            #   if [[ -z "$RESTARTS" || ! "$RESTARTS" =~ ^[0-9]+$ ]]; then
            #     RESTARTS=0
            #   fi

            #   echo "Number of restarts $RESTARTS"

            #   if [ $RESTARTS -gt << parameters.max_restarts >> ]; then
            #     echo "FAILURE_REASON='Exceeded maximum number of restarts'" > failure_reason.env
            #     exit 1
            #   fi

            #   sleep 10  # Check every 10 seconds
            # done

            # if [ $DEPLOYMENT_FOUND = false ]; then
            #     echo "FAILURE_REASON='Deployment was not found'" > failure_reason.env
            #     exit 1
            # fi     

            # if [ $REPLICAS_OK = false ]; then
            #     echo "FAILURE_REASON='Desired replicas doesn't match ready replica'" > failure_reason.env
            #     exit 1
            # fi

            # -----------------------------------------------------------------------

jobs:
  rollback-component:
    docker:
      - image: cimg/aws:2023.03
    environment:
      COMPONENT_NAME: << pipeline.deploy.component_name >>
      NAMESPACE: << pipeline.deploy.namespace >>
      ENVIRONMENT_NAME: << pipeline.deploy.environment_name >>
      TARGET_VERSION: << pipeline.deploy.target_version >>
    steps:
      - checkout
      - attach_workspace:
          at: . 

      ### Uncomment this section if you are using AWS EKS, otherwise add the steps to authenticate with your platform
      # - aws-cli/setup:
      #     role_arn: $AWS_OIDC_ROLE
      #     region: $AWS_REGION
      #     role_session_name: "example"
      #     session_duration: "1800"     
      # - run: aws sts get-caller-identity   
      # - run: aws configure list
      # - run:
      #     name: Update kubeconfig for EKS
      #     command: |
      #       aws eks update-kubeconfig --name "$EKS_CLUSTER_NAME"
      #       aws sts get-caller-identity  # Verify credentials are still valid

      - helm/install_helm_client
      - install_yq

      # This command is used to validate that the current version on your cluster matches the value that was specified when 
      # the pipeline was triggered. If that is not the case it is possible that the deployment has been updated in the meantime
      # this check is optional and can be removed if you don't need it.
      # Refer to the commands section above for details about the implementation of this command.
      - verify_current_version:
          resource_name: "<< pipeline.deploy.component_name >>"
          namespace: "<< pipeline.deploy.namespace >>"
          current_version: "<< pipeline.deploy.current_version >>"  

      # This command is used to retrieve the target revision that will be used to perform the rollback.
      # Depending on your implementation you may not need this, in which case feel free to remove it.
      # Refer to the commands section above for details about the implementation of this command.                
      - retrieve_target_revision:    
          resource_name: "<< pipeline.deploy.component_name >>"
          namespace: "<< pipeline.deploy.namespace >>"
          target_version: "<< pipeline.deploy.target_version >>" 

      # This step will create a new deploy with PENDING status that will show up in the deploys tab in the UI                     
      - run:
          name: Plan release of deploy release smoke test
          command: |
            circleci run release plan  \
              --environment-name=${ENVIRONMENT_NAME} \
              --namespace=${NAMESPACE} \
              --component-name=${COMPONENT_NAME} \
              --target-version=${TARGET_VERSION} \
              --rollback

      # This command will perform the actual rollback, using the revision retrieved by retrieve_target_revision        
      - perform_rollback:
          resource_name: "<< pipeline.deploy.component_name >>"
          namespace: "<< pipeline.deploy.namespace >>"

      # This step will update the PENDING deployment marker to RUNNING. 
      # If you are not going to perform any validation you can just remove this.                
      - run:
          name: Update planned release to RUNNING
          command: |
            circleci run release update \
              --status=RUNNING

      # This step performs validation on the deployment status after the rollback and sets the failure reason if the validation fails.
      # if you don't want to perform any validation you can just remove this.
      - validate_deployment:
          resource_name: "<< pipeline.deploy.component_name >>"
          target_version: "<< pipeline.deploy.target_version >>"   
          namespace: "<< pipeline.deploy.namespace >>"  

      # These last two steps update the PENDING deployment marker to SUCCESS or FAILED, based on the outcome of the job.
      - run:
          name: Update planned release to SUCCESS
          command: |
            # if the rollback failed, we don't want to update the status to SUCCESS. This is unnecessary if there is no logic around
            # validating the deployment status.
            if [ -f failure_reason.env ]; then
              exit 0
            fi
            circleci run release update \
              --status=SUCCESS  
          when: on_success
      - run:
          name: Update planned release to FAILED
          command: |
            if [ -f failure_reason.env ]; then
              source failure_reason.env
            fi
            FAILURE_REASON="${FAILURE_REASON:-}" 
            circleci run release update \
             --status=FAILED \
             --failure-reason="$FAILURE_REASON"
          when: on_fail  

  # This job handles the cancellation of the rollback deploy marker if the rollback job is canceled
  cancel-rollback:
    docker:
      - image: cimg/aws:2023.03 
    steps:                 
      - run:
          name: Update planned release to CANCELED
          command: |
            circleci run release update \
             --status=CANCELED         

workflows:
  rollback:
    jobs:
      - rollback-component:
          context:
            # provide any required context
      - cancel-rollback:
          context:
            # provide any required context
          requires:
            - rollback-component:
              - canceled
          filters:
            branches:
              only: main
